{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow versus Poisonous Mushrooms\n",
    "\n",
    "After the [Keras Example](01-KerasPoisonousMushrooms.ipynb), this example builds a dense neural network as a comparsion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction \n",
    "\n",
    "This example use feature extraction techniquies as the [Keras Example](01-KerasPoisonousMushrooms.ipynb).\n",
    "\n",
    "In summary, the data prep follows these steps...\n",
    "1. Load a pandas dataframe from a csv file.\n",
    "2. Transform categorial data to one-hot represention \n",
    "3. Split the training and test data sets.\n",
    "4. Extract edibility as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "srooms_df = read_csv('../data/agaricus-lepiota.data.csv')\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "mappings = ([\n",
    "    ('edibility', sklearn.preprocessing.LabelEncoder()),\n",
    "    ('odor', sklearn.preprocessing.LabelBinarizer()),\n",
    "    ('habitat', sklearn.preprocessing.LabelBinarizer()),\n",
    "    ('spore-print-color', sklearn.preprocessing.LabelBinarizer())\n",
    "])\n",
    "\n",
    "mapper = DataFrameMapper(mappings)\n",
    "srooms_np = mapper.fit_transform(srooms_df.copy()).astype(np.float32)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(srooms_np, test_size = 0.2, random_state=7)\n",
    "train_labels = train[:,0:1]\n",
    "train_data = train[:,1:]\n",
    "test_labels = test[:,0:1]\n",
    "test_data = test[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Tensorflow requies a bit more work than Keras to define the network because we need to define the models parameters (i.e. the wiegths and biases).  Here is a Keras code snippnet for comparion: \n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_dim=25))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "Here are the key differences:\n",
    "1. Tensor flow use name scoping to logically separate the layers.\n",
    "2. Each dense layer defines and initializes the weights and biases varibales (implictly done in Keras)\n",
    "3. Tensorflow doesn't use a sequential layers.  Instead, the model defines Tensor references between layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "def inference(samples, input_dim, dense1_units, dense2_units):\n",
    "    with tf.name_scope('dense_1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([input_dim, dense1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(input_dim))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([dense1_units]),\n",
    "                             name='biases')\n",
    "        dense1 = tf.nn.relu(tf.nn.xw_plus_b(samples, weights, biases))\n",
    "        \n",
    "    with tf.name_scope('dropout'):\n",
    "        dropout = tf.nn.dropout(dense1, 0.5)\n",
    "        \n",
    "    with tf.name_scope('dense_2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([dense1_units, dense2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(dense2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([dense2_units]),\n",
    "                             name='biases')\n",
    "        output = tf.sigmoid(tf.nn.xw_plus_b(dropout, weights, biases))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compile\n",
    "\n",
    "Unlike Keras, TensorFlow doesn't provide pre-canned functions for training.  The model needs the following functions defined.\n",
    "\n",
    "1. Define a loss function.  The functions convert probabilities to logits.  The clip function prevents a ```log(0)```.  \n",
    "2. Define a training functions.  Uses the loss to compute the gradients.\n",
    "3. Define a accuracy functions to messure the \n",
    "\n",
    "\n",
    "Again Keras hides these detials by prodiving precanned loss and accuracy functions.  The same definition in keras is a one liner.\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(output, labels, from_logits=False):\n",
    "  if not from_logits:\n",
    "    epsilon = 10e-8\n",
    "    output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    \n",
    "  xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=output)\n",
    "  return tf.reduce_mean(xentropy)\n",
    "\n",
    "def training(loss):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def predict(output):\n",
    "    return tf.round(output)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    return tf.reduce_mean(tf.to_float(tf.equal(predict(output),labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This entire code block represents a single line in Keras... \n",
    "\n",
    "```\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=32, callbacks=[tensor_board])\n",
    "```\n",
    "\n",
    "So, what's going on here?\n",
    "1. Define an input producer to batch samples and shuffle examples between epochs.\n",
    "2. Create SummaryWriter to write TensorBoard logs\n",
    "3. Iterate over each batches\n",
    "    * Print metrics every epoch(ish)\n",
    "    * Write out metrices every epoch(ish)\n",
    "4. Save parameters when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create saver\n",
      "session started\n",
      "loaded data\n",
      "Step 0: loss = 0.76, acc = 0.453 (0.023 sec)\n",
      "Step 100: loss = 0.33, acc = 0.844 (0.009 sec)\n",
      "Step 200: loss = 0.18, acc = 0.953 (0.009 sec)\n",
      "Step 300: loss = 0.18, acc = 0.906 (0.009 sec)\n",
      "Step 400: loss = 0.08, acc = 1.000 (0.009 sec)\n",
      "Step 500: loss = 0.08, acc = 0.969 (0.009 sec)\n",
      "Step 600: loss = 0.05, acc = 1.000 (0.009 sec)\n",
      "Step 700: loss = 0.06, acc = 0.969 (0.009 sec)\n",
      "Step 800: loss = 0.05, acc = 0.969 (0.009 sec)\n",
      "Step 900: loss = 0.07, acc = 0.969 (0.009 sec)\n",
      "Step 1000: loss = 0.03, acc = 1.000 (0.009 sec)\n",
      "Saving\n",
      "Done training for 10 epochs, 1004 steps.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_dir = './logs/tensor_srooms'\n",
    "\n",
    "num_epochs=10\n",
    "batch_size=64\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        features_initializer = tf.placeholder(dtype=tf.float32, shape=train_data.shape)\n",
    "        labels_initializer = tf.placeholder(dtype=tf.float32, shape=train_labels.shape)\n",
    "        input_features = tf.Variable(features_initializer, trainable=False, collections=[])\n",
    "        input_labels = tf.Variable(labels_initializer, trainable=False, collections=[])\n",
    "\n",
    "        # Shuffle the training data between epochs and train in batchs\n",
    "        feature, label = tf.train.slice_input_producer([input_features, input_labels], num_epochs=num_epochs)\n",
    "        features, labels = tf.train.batch([feature, label], batch_size=batch_size)\n",
    "\n",
    "    # Define layers dimensions\n",
    "    output = inference(features, 25, 20, 1)\n",
    "    loss_op = loss(output, labels)\n",
    "    train_op = training(loss_op)\n",
    "\n",
    "    # Define the metrics op\n",
    "    acc_op = accuracy(predict(output), labels)\n",
    "\n",
    "    # Initialize all variables op\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # Saver for the weights \n",
    "    saver = tf.train.Saver()\n",
    "    print('create saver')\n",
    "    \n",
    "    # Start Session\n",
    "    sess = tf.Session()\n",
    "    sess.run(init_op)\n",
    "    print('session started')\n",
    "    \n",
    "    # Load up the data.\n",
    "    sess.run(input_features.initializer, feed_dict={features_initializer: train_data})\n",
    "    sess.run(input_labels.initializer, feed_dict={labels_initializer: train_labels})\n",
    "    print('loaded data')\n",
    "    \n",
    "    # Write the summary for tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    \n",
    "    # coordinate reading threads\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    try:\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Run one step of the model.\n",
    "            _, loss_value, acc_value = sess.run([train_op, loss_op, acc_op])\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Write the summaries and print an overview fairly often.\n",
    "            if step % 100 == 0:\n",
    "                # Print status to stdout.\n",
    "                print('Step %d: loss = %.2f, acc = %.3f (%.3f sec)' % (step, loss_value, acc_value, duration))\n",
    "                # Update the events file.\n",
    "                summary_str = sess.run(summary_op)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "            step += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Saving')\n",
    "        saver.save(sess, log_dir, global_step=step)\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "    # Wait for threads to finish.\n",
    "    coord.join(threads)\n",
    "    sess.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
